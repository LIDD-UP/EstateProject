    1：17.48:
    开始爬取，使用的是
    setting 中的配置情况：
        CONCURRENT_REQUESTS = 100
    # 降低log级别：降低到INFO级别就不能获取重定向的一些信息了
    # LOG_LEVEL = 'INFO'
    # 对于不需要登陆的网站禁用cookies
    COOKIES_ENABLED = False
    # 禁止重试:对于失败的http请求取消重试；但是这个还需要考虑
    RETRY_ENABLED = False

    # 如果您对一个非常慢的连接进行爬取(一般对通用爬虫来说并不重要)， 减小下载超时能让卡住的连接能被快速的放弃并解放处理其他站点的能力。
    DOWNLOAD_TIMEOUT = 15 # 其中15是设置的下载超时时间

    # 禁止重定向
    REDIRECT_ENABLED = False
    # 设置代理ip池，可以使用downloadermiddleware
    # 配置请求头


    # 自动限速设置：
    AUTOTHROTTLE_ENABLED = True
    DOWNLOAD_DELAY = 1
    2019-01-18 17:57:04 [scrapy.extensions.logstats] INFO: Crawled 75 pages (at 0 pages/min), scraped 36 items (at 0 items/min)

2：18.27
    100 并发去掉：（使用1）可能是fiddler的原因；
    2019-01-18 18:30:40 [scrapy.extensions.logstats] INFO: Crawled 67 pages (at 12 pages/min), scraped 32 items (at 5 items/min)
3：18.27：关掉fiddler代理


4:直接重构代码，详情页不用接口爬取；
    1:使用哈尔滨的网络，参数调到最低，没过多久就封ip了
    2：使用遨游vpn，使用美国的线路，相同参数效果还算稳定；
        能够跑完，但是数据量不够，不知道那里出问题了：
            找到原因了：xpath语法的原因；
        12：13开发抓的，使用最低配置：使用遨游，抓取时间：

总结：使用哈尔滨的ip不管是抓接口还是抓dom，美国都多就封了；（使用的还是最低的配置）抓了30多个item就封了

使用遨游vpn：
    各项参数调到最低也是直接封ip（抓了284）个
    2019-01-21 12:41:26 [scrapy.extensions.logstats] INFO: Crawled 316 pages (at 33 pages/min), scraped 284 items (at 12 items/min)
    反正抓取的时间不长；

    加上referer：一样的

    我怀疑是如果禁用了cookies之后对于一个ip会有用户个数限制，如果用户量过多，就会直接封ip

    # 加上一些别的浏览器标识，禁用cookie，自动限速看一下情况：
     Crawled 83 pages (at 39 pages/min), scraped 29 items (at 0 items/min)
     果然是针对用cookie级别的封锁；
     不同的ip有用户量的限制；


新的办法，
    1：使用无头浏览器，大量获取网站的cookies，然后封完一个cookies之后再使用另外一个cookies，这样把封锁限制到用户级别，
     <div class="alert alert-sm alert-warning alert-corner alert-dismissible no-show" role="alert" id="notification-cookies">
            <button type="button" class="close" data-dismiss="alert" aria-label="Close">
                <span aria-hidden="true">&times;</span>
            </button>
            <span>It looks like Cookies are disabled in your browser. For the best experience, please enable cookies when using our site.</span>
        </div>

看了一遍最新的文章发现scrapy有自动管理cookie的功能
但是使用cookie的时候也是同样发送几十个请求之后封了ip；（哈尔冰的网络）但是agent是动态的，
注释掉动态的agent；情况：
2019-01-22 16:04:07 [scrapy.extensions.logstats] INFO: Crawled 82 pages (at 37 pages/min), scraped 31 items (at 0 items/min)
情况一样；

目前能做的可能性就是用美国的网络：在启用cookie的情况下（在要被封之前就清除cookie，重新获取cookie）不然一被封就是ip被封，不是封一个cookie
这种情况下就要获取大量的cookie；（但是同一个ip下新的cookie过多也会被封，）
所以这个网站很恐怖，
参数调到最低的情况下依旧被封；

# 接下来尝试怎样大量获取该网站的cookie以及一个ip地址下最多存在多少cookie:
使用requests库;


realtor 网站cookie的生成机制:浏览器只要一刷新,产生的cookie就和上次的cookie不太一样;但是还是有一本分是相同的;后面有一段开始就不同的,可能是前端产生的;里面还有一段搜索条件;


测试在同一个ip下:同一个cookies不限速度能请求多少次(使用python requests 库)
    接下来发生的情况:
        1:在浏览器中能够访问网站,没有被重定向到ipblock页面
        2:去掉cookie,再访问,不行
        3:去掉cookie,加上user-agent 不行
        4:再加上浏览器的一些标识之后,可以,
            然后利用这些浏览器标识和cookie,使用chrome浏览器作为user-agent,请求200多次后被封了,
            被封的方式是封的浏览器,cookie不知道有没有被封;(也就是说,优先封锁的浏览器,这很奇怪;)
                测试cookies有没有被封;
                测试发现新的user-agent + 以前被封的cookies一样能请求通过,所以,他没有针对cookie进行封锁,也就是说可以是针对user-agent的封锁
                (因为发现,用新的url去请求也是一样的)

                但是这还是不能说明问题,应为用的是requests库,反复请求同一个网站导致的封锁;


接下来在继续测试下面的假设:---------->>如果不带cookies过去,单纯的只      是夹带useragent过去;
    结果:



系统一下测试过程:
    1:分析网络请求的要素
        1:浏览器请求普遍的header
        2:浏览器请求重要的header元素
            1:user-agent
            2:cookie

#这些测试都是基于台湾125.227.36.106(这个ip的范围:125.227.30.0 - 125.227.61.255)(考虑到这个网站对不同地区的ip的流量限制不同)
    该ip ping www.realtor.com ping出的ip为(13.35.165.23)在美国;
存在问题:考虑到之前用scrapy爬取网站禁用cookie之后直接封ip,还有结合前面的简单测试发现优先封的是user-agent所以
    优先测试带cookie的情况:(接下来默认带cookie)(测试分为两组,一组是request反复请求同一个url,以及scrapy的规律爬取)
    1:request组(带cookie)
        1:使用requests啥都不加
            带cookie过去,直接用python-request作为user-agent,刚开始能够访问,没几下就不能访问了(没有直接封ip)
                用浏览器还能访问(使用的google的解析出的cookie,chrome和firefox都能访问);
            1:过一会再请求:
                重定向到userblock页面,同上面;
            2:更换cookie
                同

        2:使用requests+普通的浏览器请求header #
            也是出现问题;(直接重定向到userblock

        3:使用request+user-agent
            1:没有加在一定时间停止后再请求:
                只能请求130多次
                1:更换浏览器没有更换cookie,请求2次就被封了;可能是昨天测试的时候,这个user-agent被封过
                两个浏览器的问题:
                # 'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36',
                'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:25.0) Gecko/20100101 Firefox/25.0',
                    这里可以做个推测对于不同平台或者不同版本的浏览器的user-agent也有不同的区别.,封禁的量级不同;
                2:在换一个更新的user-agent:依然能请求很多次;
                    9.20开始:到10:00.总共2000多次请求,也就是说每分钟请求在60次左右,
                    请求次数是:60/min,这种速度情况下能够稳定抓取,但是在scrapy的情况下不一定;
                3:添加一些时间停顿
                    暂时不测,等一下再scrapy进行用request的目的在于分析网站是否对referer有限制;

            总结一:通过用requests对同一个url反复请求发现,加上cookie 和user-agent在速度60次请求/min的情况下是能够稳定抓取的;
                但是这个结论还不一定,影响因素还有可能是user-agent的版本;或者是平台;
                在之前的:    # 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',
                请求130多次也是被禁了,但是禁止的是user-agent
                    优化方向:加快爬取速度,再user-agent被封的情况下更换user-agent;
                    还有个是随时一直更换user-agent,但是之前用scrapy,直接就封了ip,(禁用cookie和启用cookie的情况下都是,没有用浏览器复制出来的cookie)
                        所以这里先做一个假设就是:这个网站有两个级别的封锁,首先是user-agent的封锁,其次是ip的封锁;(有点是错杀一千,不放过一个的感觉)

    2:scrapy 组(带cookie)(要想使用自己的cookie要禁用cookieenable)(使用接口)
        1:裸奔

        2:加一些普通的浏览器标识
        3:user-agent
        4:随机变换user-agent
        5:user-agent 被封了之后在变;同时将被302的url重新yieldscapy下载器中
    3:request组(不带cookie) (不带cookie有很大的可能直接封ip)
        1:使用requests啥都不加
        2:使用requests+普通的浏览器请求header
        3:使用request+
    4:scrapy 组(带cookie)(要想使用自己的cookie要禁用cookieenable)(使用接口)
        1:裸奔
        2:加一些普通的浏览器标识
        3:user-agent
        4:随机变换user-agent
        5:user-agent 被封了之后在变;同时将被302的url重新yieldscapy下载器中


    IIIV:还有两组是scrapy关于不使用接口的情况;
    1:带cookie

    2:不带cookie



    总结:以前把速度降到最慢还是被封ip:起始并不是封ip,只是scrapy设置错误,用的是本地电脑最新版的,所以被封了,
    但是也有可能是哈尔滨ip的问题;

   测试动态user-agent+禁用cookie的情况:
            1:在爬取一个州下的一个县的时候:(数据量只有465个的情况)能够完全获取
            2:对全局进行抓取(也就是把所有的条件放入起始请求的情况下,没过多久直接被封ip了
            这里做出如下假设:
                1:夹带cookie进行请求:(全局请求)
                    结果:在请求了160多次之后被封了ip:这里可以得出结论(结合之前user-agent固定之后被封的情况)cookie对这个网站识别没有作用,它不会封cookie,但是会
                        封user-agent,如果你的user-agent是动态变化的,那它找不到封锁的东西就直接封锁ip了;但是这个结论要测试是否跟
                            同一个ip请求的州的个数过多导致的);

                2:再测试一个州下的一个县:(基于假设:可能从一个ip请求地区差异太大导致ip被封)
                只抓一个州下的一个县的数据的时候,不太容易被封,如果进行全局的抓取,如果一个ip同时请求过很多不同州下的数据,那就可能被封
                再进行测试新的一个州中县的数据,就拿纽约州下的纽约县进行测试;



                3:关于一个ip请求流量的限制(由于我之前对https://www.realtor.com/realestateandhomes-search/Autauga-County_AL)这个url抓取过几次;

                2019-01-24 15:03:02 [scrapy.extensions.logstats] INFO: Crawled 627 pages (at 50 pages/min), scraped 592 items (at 47 items/min)
                1:可以得出以下的结论,在上面的速度下,抓取一个州下面的某一个县的数据是没有问题的;(使用动态user-agent,禁用cookie的情况下)
                2:但是如果进行全局抓取,请求了160多个请求之后就被封了ip(动态user-agent的情况)
                3:还有一种方式是进行全局抓取,不随机变换user-agent,当一个user-agent被封了之后再再更换user-agent,这样就可以将封锁限制在user-agent
                    的级别,而保护ip,并且这样还能进行提速(但是也有一定风险被封ip)
                4:还有一种情况是这个网站对json数据接口没有做限制,而对page页的url,和州县页那种实际页面进行了限制
                    1:关于这种限制的假设有可能是关于由于我们没有执行某个js验证导致的,(网页可以根据我们有没有执行某一段js代码来判断是不是人类)
                    2:还是关于对同一个ip请求多个州下面的地区导致的,(更具搜索地区来进行限制)但是可能性没有上面的大);
                    可以进行假设验证:
                        再写一个程序:
                            1:直接循环爬取搜索条件:看一下请求是不是在160多次就封ip(验证是不是有地区的限制)
                            2:再同一个州下,直接迅速请求页面(验证是不是对请求个数有限制)(或者是js的限制)

                    turlia网站的停止策略:
                    再setting里设置一个暂停信号:stop_signal;写一个middleware,当遇到302的时候就+1;加到10之后就调用停止爬虫;
                    抓josn的抓取策略:需要外部一个脚本:
                        1:这种情况下就可以利用分布式的方式进行(但是不能是同一台电脑)
                        2:利用外部脚本(再写一个main函数调用)











