    1：17.48:
    开始爬取，使用的是
    setting 中的配置情况：
        CONCURRENT_REQUESTS = 100
    # 降低log级别：降低到INFO级别就不能获取重定向的一些信息了
    # LOG_LEVEL = 'INFO'
    # 对于不需要登陆的网站禁用cookies
    COOKIES_ENABLED = False
    # 禁止重试:对于失败的http请求取消重试；但是这个还需要考虑
    RETRY_ENABLED = False

    # 如果您对一个非常慢的连接进行爬取(一般对通用爬虫来说并不重要)， 减小下载超时能让卡住的连接能被快速的放弃并解放处理其他站点的能力。
    DOWNLOAD_TIMEOUT = 15 # 其中15是设置的下载超时时间

    # 禁止重定向
    REDIRECT_ENABLED = False
    # 设置代理ip池，可以使用downloadermiddleware
    # 配置请求头


    # 自动限速设置：
    AUTOTHROTTLE_ENABLED = True
    DOWNLOAD_DELAY = 1
    2019-01-18 17:57:04 [scrapy.extensions.logstats] INFO: Crawled 75 pages (at 0 pages/min), scraped 36 items (at 0 items/min)

2：18.27
    100 并发去掉：（使用1）可能是fiddler的原因；
    2019-01-18 18:30:40 [scrapy.extensions.logstats] INFO: Crawled 67 pages (at 12 pages/min), scraped 32 items (at 5 items/min)
3：18.27：关掉fiddler代理


4:直接重构代码，详情页不用接口爬取；
    1:使用哈尔滨的网络，参数调到最低，没过多久就封ip了
    2：使用遨游vpn，使用美国的线路，相同参数效果还算稳定；
        能够跑完，但是数据量不够，不知道那里出问题了：
            找到原因了：xpath语法的原因；
        12：13开发抓的，使用最低配置：使用遨游，抓取时间：

总结：使用哈尔滨的ip不管是抓接口还是抓dom，美国都多就封了；（使用的还是最低的配置）抓了30多个item就封了

使用遨游vpn：
    各项参数调到最低也是直接封ip（抓了284）个
    2019-01-21 12:41:26 [scrapy.extensions.logstats] INFO: Crawled 316 pages (at 33 pages/min), scraped 284 items (at 12 items/min)
    反正抓取的时间不长；

    加上referer：一样的

    我怀疑是如果禁用了cookies之后对于一个ip会有用户个数限制，如果用户量过多，就会直接封ip

    # 加上一些别的浏览器标识，禁用cookie，自动限速看一下情况：
     Crawled 83 pages (at 39 pages/min), scraped 29 items (at 0 items/min)
     果然是针对用cookie级别的封锁；
     不同的ip有用户量的限制；


新的办法，
    1：使用无头浏览器，大量获取网站的cookies，然后封完一个cookies之后再使用另外一个cookies，这样把封锁限制到用户级别，
     <div class="alert alert-sm alert-warning alert-corner alert-dismissible no-show" role="alert" id="notification-cookies">
            <button type="button" class="close" data-dismiss="alert" aria-label="Close">
                <span aria-hidden="true">&times;</span>
            </button>
            <span>It looks like Cookies are disabled in your browser. For the best experience, please enable cookies when using our site.</span>
        </div>

看了一遍最新的文章发现scrapy有自动管理cookie的功能
但是使用cookie的时候也是同样发送几十个请求之后封了ip；（哈尔冰的网络）但是agent是动态的，
注释掉动态的agent；情况：
2019-01-22 16:04:07 [scrapy.extensions.logstats] INFO: Crawled 82 pages (at 37 pages/min), scraped 31 items (at 0 items/min)
情况一样；

目前能做的可能性就是用美国的网络：在启用cookie的情况下（在要被封之前就清除cookie，重新获取cookie）不然一被封就是ip被封，不是封一个cookie
这种情况下就要获取大量的cookie；（但是同一个ip下新的cookie过多也会被封，）
所以这个网站很恐怖，
参数调到最低的情况下依旧被封；

# 接下来尝试怎样大量获取该网站的cookie以及一个ip地址下最多存在多少cookie:
使用requests库;


realtor 网站cookie的生成机制:浏览器只要一刷新,产生的cookie就和上次的cookie不太一样;但是还是有一本分是相同的;后面有一段开始就不同的,可能是前端产生的;里面还有一段搜索条件;


测试在同一个ip下:同一个cookies不限速度能请求多少次(使用python requests 库)
    接下来发生的情况:
        1:在浏览器中能够访问网站,没有被重定向到ipblock页面
        2:去掉cookie,再访问,不行
        3:去掉cookie,加上user-agent 不行
        4:再加上浏览器的一些标识之后,可以,
            然后利用这些浏览器标识和cookie,使用chrome浏览器作为user-agent,请求200多次后被封了,
            被封的方式是封的浏览器,cookie不知道有没有被封;(也就是说,优先封锁的浏览器,这很奇怪;)
                测试cookies有没有被封;
                测试发现新的user-agent + 以前被封的cookies一样能请求通过,所以,他没有针对cookie进行封锁,也就是说可以是针对user-agent的封锁
                (因为发现,用新的url去请求也是一样的)

                但是这还是不能说明问题,应为用的是requests库,反复请求同一个网站导致的封锁;


接下来在继续测试下面的假设:---------->>如果不带cookies过去,单纯的只      是夹带useragent过去;
    结果:



系统一下测试过程:
    1:分析网络请求的要素
        1:浏览器请求普遍的header
        2:浏览器请求重要的header元素
            1:user-agent
            2:cookie

#这些测试都是基于台湾125.227.36.106(这个ip的范围:125.227.30.0 - 125.227.61.255)(考虑到这个网站对不同地区的ip的流量限制不同)
    该ip ping www.realtor.com ping出的ip为(13.35.165.23)在美国;
存在问题:考虑到之前用scrapy爬取网站禁用cookie之后直接封ip,还有结合前面的简单测试发现优先封的是user-agent所以
    优先测试带cookie的情况:(接下来默认带cookie)(测试分为两组,一组是request反复请求同一个url,以及scrapy的规律爬取)
    1:request组(带cookie)
        1:使用requests啥都不加
            带cookie过去,直接用python-request作为user-agent,刚开始能够访问,没几下就不能访问了(没有直接封ip)
                用浏览器还能访问(使用的google的解析出的cookie,chrome和firefox都能访问);
            1:过一会再请求:
                重定向到userblock页面,同上面;
            2:更换cookie
                同

        2:使用requests+普通的浏览器请求header #
            也是出现问题;(直接重定向到userblock

        3:使用request+user-agent
            1:没有加在一定时间停止后再请求:
                只能请求130多次
                1:更换浏览器没有更换cookie,请求2次就被封了;可能是昨天测试的时候,这个user-agent被封过
                两个浏览器的问题:
                # 'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36',
                'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:25.0) Gecko/20100101 Firefox/25.0',
                    这里可以做个推测对于不同平台或者不同版本的浏览器的user-agent也有不同的区别.,封禁的量级不同;
                2:在换一个更新的user-agent:依然能请求很多次;
                    9.20开始:到10:00.总共2000多次请求,也就是说每分钟请求在60次左右,
                    请求次数是:60/min,这种速度情况下能够稳定抓取,但是在scrapy的情况下不一定;
                3:添加一些时间停顿
                    暂时不测,等一下再scrapy进行用request的目的在于分析网站是否对referer有限制;

            总结一:通过用requests对同一个url反复请求发现,加上cookie 和user-agent在速度60次请求/min的情况下是能够稳定抓取的;
                但是这个结论还不一定,影响因素还有可能是user-agent的版本;或者是平台;
                在之前的:    # 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',
                请求130多次也是被禁了,但是禁止的是user-agent
                    优化方向:加快爬取速度,再user-agent被封的情况下更换user-agent;
                    还有个是随时一直更换user-agent,但是之前用scrapy,直接就封了ip,(禁用cookie和启用cookie的情况下都是,没有用浏览器复制出来的cookie)
                        所以这里先做一个假设就是:这个网站有两个级别的封锁,首先是user-agent的封锁,其次是ip的封锁;(有点是错杀一千,不放过一个的感觉)

    2:scrapy 组(带cookie)(要想使用自己的cookie要禁用cookieenable)(使用接口)
        1:裸奔

        2:加一些普通的浏览器标识
        3:user-agent
        4:随机变换user-agent
        5:user-agent 被封了之后在变;同时将被302的url重新yieldscapy下载器中
    3:request组(不带cookie) (不带cookie有很大的可能直接封ip)
        1:使用requests啥都不加
        2:使用requests+普通的浏览器请求header
        3:使用request+
    4:scrapy 组(带cookie)(要想使用自己的cookie要禁用cookieenable)(使用接口)
        1:裸奔
        2:加一些普通的浏览器标识
        3:user-agent
        4:随机变换user-agent
        5:user-agent 被封了之后在变;同时将被302的url重新yieldscapy下载器中


    IIIV:还有两组是scrapy关于不使用接口的情况;
    1:带cookie

    2:不带cookie



    总结:以前把速度降到最慢还是被封ip:起始并不是封ip,只是scrapy设置错误,用的是本地电脑最新版的,所以被封了,
    但是也有可能是哈尔滨ip的问题;

   测试动态user-agent+禁用cookie的情况:
            1:在爬取一个州下的一个县的时候:(数据量只有465个的情况)能够完全获取
            2:对全局进行抓取(也就是把所有的条件放入起始请求的情况下,没过多久直接被封ip了
            这里做出如下假设:
                1:夹带cookie进行请求:(全局请求)
                    结果:在请求了160多次之后被封了ip:这里可以得出结论(结合之前user-agent固定之后被封的情况)cookie对这个网站识别没有作用,它不会封cookie,但是会
                        封user-agent,如果你的user-agent是动态变化的,那它找不到封锁的东西就直接封锁ip了;但是这个结论要测试是否跟
                            同一个ip请求的州的个数过多导致的);

                2:再测试一个州下的一个县:(基于假设:可能从一个ip请求地区差异太大导致ip被封)
                只抓一个州下的一个县的数据的时候,不太容易被封,如果进行全局的抓取,如果一个ip同时请求过很多不同州下的数据,那就可能被封
                再进行测试新的一个州中县的数据,就拿纽约州下的纽约县进行测试;



                3:关于一个ip请求流量的限制(由于我之前对https://www.realtor.com/realestateandhomes-search/Autauga-County_AL)这个url抓取过几次;

                2019-01-24 15:03:02 [scrapy.extensions.logstats] INFO: Crawled 627 pages (at 50 pages/min), scraped 592 items (at 47 items/min)
                1:可以得出以下的结论,在上面的速度下,抓取一个州下面的某一个县的数据是没有问题的;(使用动态user-agent,禁用cookie的情况下)
                2:但是如果进行全局抓取,请求了160多个请求之后就被封了ip(动态user-agent的情况)
                3:还有一种方式是进行全局抓取,不随机变换user-agent,当一个user-agent被封了之后再再更换user-agent,这样就可以将封锁限制在user-agent
                    的级别,而保护ip,并且这样还能进行提速(但是也有一定风险被封ip)
                4:还有一种情况是这个网站对json数据接口没有做限制,而对page页的url,和州县页那种实际页面进行了限制
                    1:关于这种限制的假设有可能是关于由于我们没有执行某个js验证导致的,(网页可以根据我们有没有执行某一段js代码来判断是不是人类)
                    2:还是关于对同一个ip请求多个州下面的地区导致的,(更具搜索地区来进行限制)但是可能性没有上面的大);
                    可以进行假设验证:
                        再写一个程序:
                            1:直接循环爬取搜索条件:看一下请求是不是在160多次就封ip(验证是不是有地区的限制)
                            2:再同一个州下,直接迅速请求页面(验证是不是对请求个数有限制)(或者是js的限制)

                    realtor网站的停止策略:
                    再setting里设置一个暂停信号:stop_signal;写一个middleware,当遇到302的时候就+1;加到10之后就调用停止爬虫;
                    抓josn的抓取策略:需要外部一个脚本:
                        1:这种情况下就可以利用分布式的方式进行(但是不能是同一台电脑)
                        2:利用外部脚本(再写一个main函数调用)

                还有一项没有测试,就是没带cookie,加user-agent的情况如何:
                        在慢速的情况下,还算正常;台湾的vpn,
                        测试快速的情况下(哈尔滨ip)
                            没过多久(就几十个请求,直接被封)但是没有直接封ip,封的是user-agent,
                            换上新的user-agent,能够访问,但是请求几次又被封,
                            所以哈尔滨的ip不能用;
                            换成台湾vpn2,原来的就又可以了,
                                当不加cookie的情况下:单一的一个user-agent是能够在慢速,就是挂vpn的情况下跑的,但是,加上
                                    cookie 之后,又是请求

            这个网站是ip加user-agent的封锁;
            如果有user-agent 就封user-agent,没有就直接封ip,cookie还需测试:
                    换上台湾的ip能够稳定运行,但是由于网速原因,跑的有点慢,

            流量到达200kb的时候就会被封,台湾的ip
            但是用没过的ip开启3个,网速也达到了200kb,但是没有被封;


            使用美国3的vpn,开启3个进程的情况下
                1:抓取速度和下载速度没有提升,一直是在20kb以下,
                    1:对于多进程的理解不够深入,程序写错了,没有充分利用带宽;
                    2:vpn的速度问题
                2: 这个user-agent封禁有点厉害:User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:29.0) Gecko/20120101 Firefox/29.0
                3:关于程序中依然存在的问题:
                    1:其中一个遇到302将请求返回的middleware有问题,返回的时候它请求本身的user-agent没有变化,
                    2:第二个关于处理302错误的也一样有问题,还需要调试;
                        1:对于多个进程的爬虫,共用一个middleware(302),程序共用一个user-agent列表,当遇到问题的时候可能会出现一样的情况)
                            这个时候就相当于设置con-request为多个的情况了,所以需要修改程序使用不同的user-agent



                    在爬取美国所有的数据的时候的问题i:
                        1:使用动态user-agent+禁用cookie的情况:(美国的ip)
                            出现160多次请求之后就不能再请求
                            或者是次数更多但是直接封的ip
                        2:当使用不变的user-agent(韩国的ip)user-agent为:User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; .NET CLR 1.1.4322)
                            请求了200多次就被被禁用了,

                        综合1,2点,以及目前正在抓取的数据来讲:
                            1:如果频繁的访问包含html的页面,会被封ip或者是user-agent,(请求频次的反爬)
                            2:html的动态页面反爬(js文件加载的问题),如果发现请求很多次都没加载js文件,直接人为是爬虫
                            3:侧面反映出json数据接口的反爬比html的反爬要更加松一些;
                            4:也就是说,在后面的爬虫中要尽量避免在一分钟内过多的请求太多次数的html
                            5:加上cookie之后,同样的存在这个问题;所以,这个网站cookie基本没用
                                并且把爬虫的参数调到最低依旧会出现相应的问题;

                            6:去掉referer然后加上cookie的情况要更好一些:
                            custom_settins:
                                custom_settings = {
                                   "ITEM_PIPELINES": {
                                        'AmericanRealEstate.pipelines.StatisticRealtorHouseCountPipeline': 301,
                                        # 'AmericanRealEstate.pipelines.RealtorDetailDomPipeline': 302,
                                        # 'AmericanRealEstate.pipelines.RealtorHouseInfoTestPipeline': 302,
                                        # 'scrapy_redis.pipelines.RedisPipeline': 300

                                    },
                                    "DOWNLOADER_MIDDLEWARES":{
                                    # 'AmericanRealEstate.middlewares.AmericanrealestateDownloaderMiddleware': 543,
                                    #  'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
                                     # 'AmericanRealEstate.middlewares.RandomUserAgentMiddleware': 543,
                                     #    'AmericanRealEstate.middlewares.Process302Middleware' :544,
                                     #    'AmericanRealEstate.middlewares.AlertUserAgentWhenEncounter302Middleware': 545,
                                },
                                    "DEFAULT_REQUEST_HEADERS": {
                                            'authority': 'www.realtor.com',
                                            'method': 'GET',
                                            # 'path':'/sitemap/Alabama-real-estate/',
                                            'scheme': 'https',
                                            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,',
                                            'accept-encoding': 'gzip, deflate, br',
                                            'accept-language': 'zh-CN,zh;q=0.9,ja;q=0.8',
                                            'cache-control': 'no-cache',
                                            'upgrade - insecure - requests': '1',
                                            'cookie': '_ss=1366x768; threshold_value=89; clstr=v; clstr_tcv=54; split_tcv=60; __vst=fd0f0f5d-d57b-48c8-84f7-ee9332f86618; __gads=ID=c9c6bc68343b1c38:T=1547191429:S=ALNI_MbEoOR211jKBxjj3c6AxORc9rNejw; _gcl_au=1.1.462908694.1547191500; ajs_user_id=null; ajs_group_id=null; _ga=GA1.2.1809646443.1547191147; __qca=P0-1856484682-1547191544959; ajs_anonymous_id=%22db0f4b00-cb6b-4f3b-b074-13c91af6e735%22; _ncg_g_id_=4911c6ac-7925-4d04-8b99-bce40fcc0b74; AMCV_8853394255142B6A0A4C98A4%40AdobeOrg=-179204249%7CMCIDTS%7C17919%7CMCMID%7C78470061863700216203027265384897932330%7CMCAAMLH-1548725294%7C11%7CMCAAMB-1548725295%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCCIDH%7C-549291961%7CMCOPTOUT-1548127694s%7CNONE%7CMCAID%7CNONE; _gid=GA1.2.1644294370.1548120496; _ncg_id_=1682b97e315-51c4ce99-0b7f-4994-8781-14746f45fe7f; __ssn=dae48bd1-8575-48ac-9d51-27bd07bacfca; __ssnstarttime=1548221607; userStatus=return_user; automation=false; split=n; bcc=false; bcvariation=SRPBCRR%3Av1%3Adesktop; header_slugs=gs%3DAdair-County_MO%26lo%3DAdair%26st%3Dcounty; ab_srp_viewtype=ab-list-view; criteria=loc%3DAdair+County%2C+MO%26locSlug%3DAdair-County_MO%26lat%3D40.190562%26long%3D-92.600719%26status%3D1%26pg%3D1%26pgsz%3D48%26sprefix%3D%2Frealestateandhomes-search%26city%3DAdair+County%26state_id%3DMO; srchID=be158ecc1301418f8a3b881bc47a8bc5; _ncg_sp_ses.cc72=*; _fbp=fb.1.1548229728818.1374132530; AWSALB=/bTmyIu5nlvbnACPmfjR1nAT2QtPGZjWun45It7FAVGsHuftFBvIWLc+38t9pAsLy2yUqUXMNpha1iMQGBv+ATeWIch7JhWPNybKy8LzVYGMFSQiF6Z8kJfP3hHV3m40228anyBezHl3NwQiHhK4s/nNH5zibgotk1hn86s0opoCavwjXQTIiP5doylcUQ==; _rdc-next_session=dDNod0Nvd1ZDdEZDV0xQV242U2lYdTI0aVZKR1J3dkN2MTkzL055RzVmMGFxS0xML2xFeXhZNHpOMnZadkVUNU5lUDlTU0xuU0d3VTh6ajlJTEN2MGozcHNDc1VUOGhEQUZMb2VSN2dkUHV1UVI1SDhFTm1QR0M2SHRQek1udHVwZjV2cmpvdW5TVkNlcXBxNTVLUTVLcmQxbUJzL0dqcXF3ZzBwZWF5SmVtUXZHL0E5dW4wUDM0V1hWSThHSGd2SmN3TFpPNXY5NkJkNno0OGNPWEFyQnVqUXJaUWRjcE5SUXNyaWMvb2NwTG0rbklXZ0hKeFEyWUJzRnFNclY0WS0tRHMzZDFtcjNEKzBWTDhTWUtwVVRjQT09--7f95f7bded29d4ea1340c765bec95c793a17ace9; _ncg_sp_id.cc72=c8b8f168-bbc0-4797-b97c-384bcd5da00d.1548122553.5.1548231603.1548143865.7e020ece-e650-47b8-a75b-5cc6c9b51159; _4c_=fVLbctowEP2VjJ5j0P3CGyGdJm0JhCSlfWKEJYOLgz2ygZDAv3cF5DJtp36Qd1dnd492zwvazP0SdYjgmjJilJSanaOF39ao84LSKp7reKxCgTpo3jRV3Wm3N5tNK3hbNGVopeVjO9q%2Bbmzj7dLNy0dfJ7W3IZ23u87mIemVq2WznfQH6BylpfNQipiWalHwm2fwuMZgVqF0q7SZNNsqQjZ%2Bela7BVx8LstZ4a8vIZg5nOFMuMQJNU24TnWieaYS7w1jNNNSEg0Z3%2FM6B3aHlIQoQzGn3Oz6vevL%2B7sdBEh0%2BteXO6W5whiyJIM%2FJZJihqmiUjDNtVGGUcYwgLvd%2FrerJI5KUUEN3xFyjF68B8Vu9HVejZ71Isxo820gq9nqx7gSZbHopg%2FL%2FPbn8Mr2x%2BOZ%2B8Kenofj20f3C2%2BhDPC62iWCGwrMZCw8GN4PHu4PpQlV0vB6dzO4%2BRRbAutoxuHNg6%2FnZeEma1us4tC0gXA9Wedgv89hchgE%2BBe29ql9hMWiG3Cn6dqG3DZ5CSpAd6PhRW806qxJx%2Fl60ZRV3FdRNwEu1xEeyk3to9ebB1jzmSIQLUEsaJwvHVyCG3zmQzigIpW8ibQ%2BqOUUBJG9x%2BPy11EVDKyiTEFO4IE0YffdycOBPdHYSC45Zy0YiiKGEK7Q%2Fhw9HQXMAICZpBTG0oBateQ4foAIuTspGWWWWDwVTmdcS0UJMZ5IlWEq0sx64tCpnhAauhktpNgfyR3yCfuzH%2FlHv%2BOc%2FpNE%2Bd9JVfEKf0NTJjEDGuKEJvwNHRd8fBHmYioFyaS2GuvUeuG1YsSxjOM0Swn6UI5xzIR5bR65xGr7%2FW8%3D',
                                            # 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',
                                            # 'user-agent':'Opera/9.80 (X11; Linux x86_64; U; fr) Presto/2.9.168 Version/11.50',
                                            # 'user-agent': 'Opera/9.80 (X1u) Presto/2.9.168 Version/11.50',
                                            'User-Agent': 'Mozilla/5.0 (Winds NT 6.1; WOW64) ApKit/56 (KHTML, like Gecko) Chrome/27.0.14.93 Sa7.36',
                                            # 'User-Agent': start_user_agent,
                                            # 'referer': 'www.realtor.com',
                                            },
                            使用的user-agent:
                                在这种情况下虽然没有停止:但是出现了301错误:
                                    2019-01-28 14:31:59 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <301 https://www.realtor.com/realestateandhomes-search/Hills-County_FL>: HTTP status code is not handled or not allowed
                            /realestateandhomes-search/Saint-Francis-County_AR
                            /realestateandhomes-search/Saint-Francois-County_MO
                            Saint-Francois-County_MO
                            /realestateandhomes-search/Saint-Charles-County_MO
                            /realestateandhomes-search/Saint-Francis-County_AR
                            Saint-Lucie-County_FL
                            /realestateandhomes-search/Saint-Johns-County_FL
                            /realestateandhomes-search/Saint-Clair-County_AL
                            /realestateandhomes-search/Hills-County_FL
                            https://www.realtor.com/realestateandhomes-search/Saint-Bernard-County_LA>

                            这里就牵涉出了一个大问题:
                                关于referer + cookie + user-agent的组合问题;


                             等一下依旧用这个爬虫进行测试,看一下问题出在那里,6种组合方式:排除肯定要user-agent的方式,就只有4中方式:
                                1:编码这三种方式:user-agent cookie referer
                                2:user-agent没有的就不用测试了:肯定不行
                                3:100:
                                    结果,能够稳定的跑完3000多个请求

                                4:101:
                                    结果:能够稳定跑

                                5:110:
                                    结果,能够稳定的跑完3000多个请求

                                6:111::
                                    在韩国的ip下竟然能正常运行,奇怪,用的是假的user-agent







                            7:解决办法:加上下载延迟;降低它每分钟的请求次数
                            8:realtor 网站大致有2043219条数据;



关于middleware中的一些问题:(编码问题:):
    1:怎样再重试302请求
    2:怎样在middleware中重试302请求
    3:怎样在middleware中重试请求并携带新的header
    4:怎样才能使我其中的一个处理302 middleware中新设置的user-agent起作用;而不至于更换之后依旧禁止请求
        没起作用的猜想:
            1:没有设置沉睡时间:原因:还是同一个http连接(但是根据http协议应该不是)
            2:设置延迟时间之后依旧没有起作用
            3:可能是假的user-agent导致的错误;


爬虫是一个i0密集型的操作,可以采用多线程的方式进行运行:
    目前用多线程+proxy 代理依然存在的问题:
        1:多线程方面:
            多线程的使用以及多进程的使用,在python中的区别,以及选择哪一个更优,对于这个项目
            多线程启动scrapy会不会有变量上的问题;
        2:之前存在的爬虫方面的问题:如何向爬虫传递参数,在多进程的程序中只能复制粘贴很多个spider,然后进行运行,这样很不科学;
            1:再次尝试能不能给爬虫传递custom_settings的参数(之前测试是只能传递url参数,传递的custom_settings不起作用;
            2:如果上面的不能成功,能不能考虑传递一个meta信息,然后将header中的user-agent改变
            3:上诉两种方法都不成功的情况下:考虑随机的情况,这样就可能让爬虫起始的user-agent不一样;

            4:上诉不成功的情况下就只能将复制粘贴代码了
            5:还可以使用的是随机的user-agent,但是这样很容易被封ip,但是考虑到如果使用代理ip的话,就直接随机效果可能更好,不需要在
                custom_settings中设置default_request_headers 了
        3:关于分布式的解决策略(这个在多进程中可能好使,但是多线程可能有问题,多线程可能会共用变量;)
            1:开启多个进程,使用scrapy-redis,然后将搜索条件全部插入到redis中,
                2:这里有个问题:如果代理ip足够多的情况下,可以开很多的抓取进程;

        4:关于代理ip middleware的问题:
            1:ip的有效性判定
                1:判定策略的把控
                    1:延迟(需不需要在高延迟的情况下判定为无效的)(即timeout的设置)
                    2:400+ 错误以及500+错误的问题是否应该判定为ip无效
                    3:对于300+错误的处理(因为被封网站时候就会被重定向),所以出现这个错误的时候可以换ip
                        但是也可以不换,但是要考虑之前的usero-agent是动态变化的还是固定之后被封了之后再变;
            2:ip变换策略

            3:ip抽取策略

            整个ip代理过程如何过程化;


1:起始现在需要解决的问题还是关于向scrapy传递customsettings的问题;如果不能传那么就可能存在被封ip的情况;
2:关于default_reqeust_headers的问题:
    1:在middleware在处理出现302码出现的问题:(优先级的问题)以及语法问题;
        1:request.headers['User-Agent'] = 'my user agent hahaha',这个设置只对当前的这个url起作用,对整个项目的default_user_agent 没有作用;
        所以最好不要在custom_settings或者settings中设置user-agent;因为设置了这个之后,就会一直存在于整个项目中的middleware中,对于所有的请求都会经过这个中间件,如果想要覆盖掉原来的user-agent就必须要让新的
        中间件的优先级高于scrapy内部的 'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,
        这个middleware,会在每一次的请求之中都会运行:
        那么在之前的会覆盖掉系统的,所以没有用,
            1:最好将user-agent从default中分离出来
            2:在middleware中的process_request 维护一个user-agent,同时需要一个变量,在在process_response中遇到302 时候将他值为true,传到process_request中;
            3:需要维护一个user-agent 列表,遇到302 之后将该user-agent 保存到一个新的文件中;
            4:随机获取一个user-agent 遇到302 之后在变化;

        遇到的一个问题:中间件中设置的default_headers不会一直有效,所有的request都要经过中间件的处理,也就是说一次性变化的header是只对当前的请求
        有作用的:
            解决办法:(default_request_header中是不能设置user-agent的,只能设置一些通用的设置)
                1:process_request中维护一个固定的index数组然后从中取出一个:但是随之而来的问题就是在开启多进程的时候需要复制很多spider进行运行,代码冗余;
                2:如果采用随机的方式获取,这个方式可以极大的简化代码,但随之而来的是提高了被封的风险(一个ip下是有用户限制的,(这里的服务器应该是将一个user-agent看成一个用户),如果用户过多会导致直接封ip)

                这里暂时只考虑维护一个固定的user-agent;同时加大更换user-agent的sleep时间


                经过测试:得出,开启三个进程的时候爬取速度能达到2万一个小时,使用的是日本2的节点;
                下载测试继续增加进程的状况:
                    1:直接怎加到20个进程,速度达到300kb-400kb每秒的情况;
                    2:9.20开始抓的;
                    3:




# 现如今需要解决scrapy的使用问题:
    1:最需要解决的就是断点续爬(也就是爬虫的停止(停止的时候如何保存爬取状态,)重启的时候从断点处重新开始;
        # 这里可以用之前的程序测一下scrapy自带的停止策略;
        这个很重要,牵涉到最终的部署问题
        1:使用scrapy 自带的任务作业
             # "-s",
             # "JOBDIR=crawls/realtor{}".format(num),
             但是这种方法存在的问题是需要在终端中输入ctrl c 信号才能停止爬虫,这种方式的停止才会保存完整的断点,其他方式的可能存在断点不完整的情况,还有一种是发送一个信号(但是没有理解这里的信号是什么)

        2:使用scrapy-redis来运行:
            但是存在如下疑问:
                1:对于断点有没有记录(需要了解scrapy的过滤是如何实现的),
                2:对于相同的爬虫,爬虫里面yield的请求会不会再返回到redis中保存;

    2:代理ip列表和user-agent存储的优化
        1:user-agent 的存储问题
            1:可以存在一个文本中;然后导入成一个列表,但是没有必要存在一个数据库中
            2:proxy_IP 的处理策略:存在数据库中,需要一个脚本定期爬取,然后检测ip是否有效然后弄一个字段来确定是否有效:
                然后在使用的时候将有效的查询出来存成一个全局的list,然后使用,如果超时或者不好使就从列表中去除;
    3:多线程和多进程开启scrapy程序测试问题:
        1:多进程目前运行正常,
        2:但是多线程不知道怎么样


    4:部署问题:
        1:部署策略:
            1:scrapyd
                1:需要解决如何向爬虫传递参数(这里是user-agent和proxy_ip)
            2:scrapy_redis
                1:但是不知道当所有的爬虫程序都停止之后能不能实现断点续传;(可能需要自己实现)
            3:docker部署(最好是用docker将爬虫容器化)

    5:爬虫中间件问题
    6:最终程序(代码)的优化问题:



realtor 网站反爬升级问题：
    1：之前的禁用cookie+变化user-agent已经出现问题：现在是直接封的ip
        出现这个问题可能的原因：
            1：对于被封过的user-agent，如果再次使用可能很快被封；（应该是有敏感度检测）
    2：当同时出现很多个敏感user-agent一起抓取的时候，会直接封ip，

    3：当我用一个新的user-agent的时候，能够稳定抓取1000多次（还在继续抓取），当
    4：还有一种可能是使用的user-agent string 不合规范，所以被直接封ip了，

    这个user-agent 抓取了2000多次之后依旧再抓取，
    User-Agent: Mozilla/5.0 (Windows NT 7.0; WOW64) AppleWebKit/501 (KHTML, like Gecko) Chrome/28.0.1547.62 Safari/501

    解决办法：需要研究各个浏览器的格式规范，生成user-agent池；
    这个依旧是不能解决问题：
        1：10，20个的情况下依旧还是不能稳定抓取，
        2：单个进程的情况下带cookie和不带cookie 的情况下都能够稳定抓取，（带cookie的情况下稳定抓取了2000多次没有问题）
        3：接下来尝试：多个进程带cookie的情况：
        /realestateandhomes-search/Saint-Helena-County_LA 这个会出现重定向问题；

        目前来看带cookie的情况相对于不带cookie的情况要相对稳定一点，但是不知道是由于是不是美国的网络导致的还是，（网速慢或者是使用的美国的vpn）携带了cookies导致的；
        看一下能否跑到1000多条；（虽然说开了10个进程，但是速度还是在50kb左右）

        然后继续用台湾的vpn跑:无论是在10还是在20个进程的情况下，都是被封，
            出现这个问题可能的因素有两点：
                1：网速较快，达到了100kb/s
                2：进程较多，当个ip下同时请求的用户过多，（可能是当前ip的用户请求个数异常检测，亦或者是浏览异常检测，相对于原来的通过user-agent有所 提升，限制）；
                    也就是说这个限制在原来的限制单个用户（user-agent）的并发，到单个ip下的并发（可以理解成单个ip下的用户个数）（也可以想成进程个数）

                 接下来就需要验证到底是网络问题（当个ip下的访问频率和流量限制问题）还是用户个数限制，感觉用户限制更大，因为请求的时候也会消耗大流量；


                 还有一个可能就是当前一个ip下一个user-agent被封之后，如果有另外一台电脑进行连接，这个封锁可能会解除，不知道是不是巧合）‘

                 在美国vpn（网速慢的情况下，请求不是很快，10个进程（带cookie的情况下，没有出现问题）

                 现在封锁是一被发现，就直接封ip，更换user-agent没用（在多进程的情况下，被发现也是一起被发现的）

                 在美国ip快速的情况下：大致100kb左右，能够顺利抓取，这里可以大致推测一下是不同地区ip导致的用户个数封锁；（也就是进程个数）
                 现在继续测试一下别的vpn下的ip：广东的ip也能和美国ip一样在cookie和10个进程的情况下也能大致稳定抓取，
                    这里可以推测一下，出现过被封情况的ip，如果被封过之后，可能会被标记，如果再次出现快速的抓取就会直接被封；在用户层级的封锁，如果你是单进程情况下抓取也不会被封；
                    可能和网段的封锁相关，就跟公司重启路由器一样，继续抓取依旧被封；


                    继续用20进程测试美国和广东ip：
                        广东1：20进程的情况下也是被封；速度达到800kb的情况：
                        广东3：加上一定的时间沉睡也是一样的；速度高峰值也达到了将近1m
                        美国6的情况：能够承受瞬间1m/s的速度，能够稳定抓取1000次（所有进程）

                        所以这里可以间接的得出结论，对于不同地区下ip（国家或者地区）的流量和用户的限制是不同的，而使用美国的ip限制要相对弱一些，使用中国，香港，日本，韩国，等地的ip，限制较强，
                        在使用中国广东的ip：能够承受10个进程的速度；如果使用20个进程就直接挂了，如果使用香港，日本，韩国的ip，会限制的更厉害，但是单进程是能够跑的，但是如果被发现直接就封ip，
                        不会在先封user-agent，（之前是先封user-agent），还有一种可能是之前被封过，所以继续抓取的时候会被盯住；

                        但是使用美国6的ip，也只能够坚持请求6000多次还是被封了IP 104.192.81.202
                        也可以测试一下不带cookies的情况：


                        测试方式：8进程+随机停止3秒+禁用cookie+遇到302错误之后更换user-agent:
                            1：结果，能够稳定抓取18万条数据：
                            2：user-agent 出现了被封的情况，但是程序没有直接挂，所以随机停止是可行的一种方案，还有减少速度之后也是一种可行的方案，
                        接下来的测试方法：
                            1：要想48小时内跑完数据，最低要求是每小时6万条数据，没秒钟要达到16条数据(每条数据8kb左右）也就是说每秒的速度是0.1-0.2m左右；
                            2：那么进程数应该是30个，加上随机停止，那么概率是0.5，所以最后是每次15个进程左右，（平均）
                            3：其他条件不变：
                            4：测试情况还包括启用cookie，随机获取user-agent;

                            5:在此之前需要完善一下代码
                                1：对于url的保存，不保存详情页的
                                2：采用多线程进行爬取；


                            关于30个进程加随机停止3秒+禁用cookie+遇到302停止10分钟的策略：
                                结果：花了大约10个小时的时间大致抓取了63万的数据，然后停了，更改user-agent没有作用；
                                    出现这种结果可能的原因有：
                                        1：发现302之后停止的时间短导致的
                                        2：爬取数据量到达一定量之后被发现后，全部被封了
                                        3：某段时间的爬取速度过高（可能性不大）
                                        4：也就是说

                                        解决办法：
                                            1:在累计爬取一定时间之后停止一个较大的时间（因为之前也做过单个进程的时间停止，所以要全部的爬虫在一定时间的时候全部停止）
                                                停止时间就设置为每3个小时停止半个小时；（这应该有一个阈值，可能是单ip每天，或者是多少个小时数据量的大小）；需要不断的调试；
                                            2：增加被发现之后的延迟时间：增加到30分钟
                                            3：随机获取user-agent，其实随机的user-agent+禁用cookie是一个比较好的方案；
                                            4:还可以动态的变化头信息；这个以后做；先做前面三个；
                                            5:随机停顿应该放在process_request中


                                            上面的方案都做了之后还是蹦了：台湾的；





找手机的上的api接口


新的抓取策略：既然知道详情页得接口
https://www.realtor.com/property-overview/M7076150985

那么我们可以直接构造property，这样的话请求数量会增加很多不和理，那么我们可以先将property_id 全部获取到了之后再进行爬取
这需要测试一下全部获取完全所需要的时间：

看一下直接获取json有没有问题；

但是这种方式也无法解决单个ip一定时间内访问量的问题，还有就是访问频率的问题；


上面是浏览器端的问题；
下面是浏览器端的接口

关于realtor json数据的进一步分析：
    查找关于link的关键字
    查找到了有similar的url 可以获取多条数据，但是获取的数据不全，并且显示还有限制；所以没用；

    但是详情页的接口是可以用的：
        https://mapi-ng.rdc.moveaws.com/api/v1/properties/3796179283?
        listing_id=616647169&prop_status=for_sale&schema=legacy&client_id=rdc_mobile_native%2C9.3.7%2Candroid

        最重要的是最后的一个client_id 信息，
        刚开始爬取的时候可以用最原始的进行爬取：
        关于头信息：
            Cache-Control: public, max-age=7200
            Mapi-Bucket: for_sale_v2:on,for_rent_ldp_v2:on,for_rent_srp_v2:on,recently_sold_ldp_v2:on,recently_sold_srp_v2:on,not_for_sale_ldp_v2:on,not_for_sale_srp_v2:on,search_reranking_srch_rerank1:variant1
            Host: mapi-ng.rdc.moveaws.com
            Connection: Keep-Alive
            Accept-Encoding: gzip
            User-Agent: okhttp/3.10.0


现在的问题是批量获取property_id 并且通过详情页的json数据接口获取数据：

如果通过网页获取的话，他的sale和rent的子域名是再不同地方的，所以如果通过网页获取propery_id ,其实爬取量还是有一点大的，
如果通过app的话，一下能返回200条数据，可以考虑用app上的接口获取propery_id

但是目前的问题是关于信息构造的问题；

可以先测试一下：用的是以前被封过的ip，一下就被封了：

现在一个方案是将property_id 分开：

但是不知道移动端的极限在哪里：可以直接用自己构造的url进行爬取




最新抓取策略步骤：

    整个流程的过程：
        1：列表页的抓取（每天都要进行）（10小时内完成）
        2：存入到数据库中形成表1：realtor_list_page_json_data
        3:拆分realtor_list_page_json_data 形成表2：realtor_list_page_json_data_splite
        4:如果是第一次，就直接将拆分的数据再存入到表3：judge_with_realtor_list_page_json_data_splite_to_decide_which_need_to_update
            如果是第二次就需要跟表judge_with_realtor_list_page_json_data_splite_to_decide_which_need_to_update 对比property_id 字段，last_update字段，address字段
                字段说明：
                    1：property_id：对比判断当前id是否存在，若存在在进行判断last_update 字段和address字段是否相同，若不同则将详情页数据表：realtor_detail_page_json_data 中的is_dirty 字段设置为1
                        若不存在则将该property_id 添加到judge_with_realtor_list_page_json_data_splite_to_decide_which_need_to_update，同时在将该property_id添加到realtor_detail_page_json_data的Property_id字段
        5：realtor_detail_page_json_data
            对于该表，如果是第一次，就将judge_with_realtor_list_page_json_data_splite_to_decide_which_need_to_update 表中的property_id 插入到该表中，其实第4步完成了该功能；

        6：然后在详情页抓取策略：
            判断realtor_detail_page_json_data 表中的is_dirty是否为1 和jsonData字段是否为空：如果是这两个其中一个条件满足，就将该字段的property_id 提出来之后作为爬取条件；
        7：对于新爬取的数据就直接插入，应该是更新该字段，新字段的产生都通过列表页的抓取提供，更新字段包括，jsonData，is_dirty为0

        整个流程就是这样，最后就是要解决开发问题：
            1：最重要的就是爬虫爬取的限制问题解决方法；（通过代码的方式展示middleware中）
                由于当前不知道接口有没有user-agent限制，所以不做user-agent的变化；
                1：列表页的策略；
                    速度：10小时内：
                        1：单个并发，然后设置随机停止5秒，需要传入一个起始时间，再抓取了2个小时后，停止半个小时；
                        2：4个并发，然后设置随机停止5秒，抓取了半个小时后停止10分钟

                2：详情页的抓取：
                    1：单个并发，然后设置随机停止5秒，需要传入一个起始时间，再抓取了2个小时后，停止半个小时；
                        2：4个并发，然后设置随机停止5秒，抓取了半个小时后停止10分钟



    1：利用列表页的抓取接口返回所有的property_id(大约1w~2w）（抓取的时候稳一点）


        关于再抓取策略的问题：
            1：首先先抓取列表页的数据
            2：
        抓取的数据的处理方式：
            拆分字段：（可能需要用到存储过程）
                关于列表页数据表的问题
                    1：房屋的基本字段
                    2：property_id  字段（用于作为详情页接口进行抓取数据）（数据接口包括两个）
                    3：last_update 字段 ：（用于检测新抓取的数据和当前数据更新时间对比，如果时间变化，就将该数据作为脏数据，并将is_dirty 字段更新为1；
                    4：新建一个is_dirty 字段：如果该字段为1，则表示当前为脏数据，需要重新抓取详情页的信息；抓取了详情页的信息之后将该字段设置为0；
                关于详情页数据的数据表的问题：
                    1：整体的json数据，
                    2：

    2:利用






realtor 新的流程：

list——page页面的抓取：
每次的搜索条件一样
1：可以全部抓完之后再做后续的工作，也可以抓一条之后就进行处理，先弄边抓边处理的流程。

2：detail 页面的数据抓取：
    1：从psql数据库中读数据，然后传入spider中进行抓取，再抓取的过程中需要注意保存抓取了和为抓取的url（保存到txt文档中），这样方便处理；
    2：从psql数据库中读取，然后放入到redis中，那么这样的话就需要做成scrapy-redis了，那么这个时候就可以将抓取过的放入redis中的一个key中，没抓取的放到一个key中，不用用txt保存；
    3：对数据还需要进行修改，需要加入操作时间信息
    4：对程序进行修改，可以利用web 和app两个接口进行抓取，提升速度，但是这里存在的问题是web端已经没有测试ip了，现在慢速情况下也会被封ip。





sql 语句：
-- INSERT INTO realtor_list_page_json_splite ( "propertyId", "lastUpdate", address ) SELECT
-- json_array_elements ( "jsonData" -> 'listings' ) ->> 'property_id' AS "propertyId",
-- json_array_elements ( "jsonData" -> 'listings' ) ->> 'last_update' AS "lastUpdate",
-- json_array_elements ( "jsonData" -> 'listings' ) ->> 'address' AS address
-- FROM
-- 	realtor_list_page_json

-- 找到有的propertyId
-- SELECT
-- 	rl."propertyId" AS "listPropertyId",
-- 	rl."lastUpdate" AS "listLastUpdate",
-- 	rl.address AS "listAddress",
-- 	rd."propertyId" AS "detailPropertyId",
-- 	rd."lastUpdate" AS "detailLastUpdate",
-- 	rd.address AS "detailAddress"
-- FROM
-- 	realtor_list_page_json_splite rl
-- 	INNER JOIN realtor_detail_page_json rd ON rl."propertyId" = rd."propertyId"

-- UPDATE realtor_detail_page_json set "isDirty"='1'
-- WHERE "propertyId" ='6264702487'


-- 找到没有的propertyId

-- INSERT INTO realtor_detail_page_json( "propertyId", "lastUpdate", address,"isDirty")
-- (SELECT
-- 	rl."propertyId" AS "listPropertyId",
-- 	rl."lastUpdate" AS "listLastUpdate",
-- 	rl.address AS "listAddress",
-- 	0
--
-- -- 	rd."propertyId" AS "detailPropertyId",
-- -- 	rd."lastUpdate" AS "detailLastUpdate",
-- -- 	rd.address AS "detailAddress"
-- FROM
-- 	realtor_list_page_json_splite rl
-- 	left JOIN realtor_detail_page_json rd ON rl."propertyId" = rd."propertyId"
-- 	WHERE rd."propertyId" is NULL
-- 	and rl."propertyId" is NOT null
-- 	 and rl."lastUpdate" is NOT NULL
-- 	 and rl.address is NOT NULL
--
-- 	)





-- SELECT "propertyId" FROM realtor_detail_page_json
-- WHERE "detailJson" is NOT NULL
-- AND "isDirty" ='1'

-- UPDATE realtor_detail_page_json set "detailJson"='{"aa":"11"}' ,"isDirty"='0'
-- WHERE "propertyId" ='6264702487'
--
-- update realtor_detail_page_json set "optionDate"=now()



app api :100kb 以内的网速，国内广东2vpn，10个并发，40分钟跑了14000条，没有被封ip，
所以可以这样做：爬取1个小时之后停10分钟继续跑，就以之前的速度跑，



history_sql :
UPDATE
realtor_detail_page_json rj
set
"isDirty" = '0', "lastUpdate" = tmp."lastUpdate", address = tmp.address
FROM(
values
('6610600259', '2019-01-22T09:03:06Z', '767 Pinehaven Cir in Pine Haven, Clover, 29710', '6610600259', '2019-01-22T09:03:06Z', '767 Pinehaven Cir in Pine Haven, Clover, 29710'),('7800210333', '2018-08-16T18:01:53Z', 'Spur Five Rd, Jump River, 54434', '7800210333', '2018-08-16T18:01:53Z', 'Spur Five Rd, Jump River, 54434'),('3357926695', '2019-01-31T15:01:13Z', '128 Cold Spring Point Rd in Tuckahoe, Southampton, 11968', '3357926695', '2019-01-31T15:01:13Z', '128 Cold Spring Point Rd in Tuckahoe, Southampton, 11968')
 ) as tmp("propertyId", "lastUpdate",address,aa,bb,cc)
 WHERE rj."propertyId" =tmp."propertyId"



 -- INSERT INTO realtor_list_page_json_splite ( "propertyId", "lastUpdate", address ,"optionDate")
-- SELECT
-- json_array_elements ( "jsonData" -> 'listings' ) ->> 'property_id' AS "propertyId",
-- json_array_elements ( "jsonData" -> 'listings' ) ->> 'last_update' AS "lastUpdate",
-- json_array_elements ( "jsonData" -> 'listings' ) ->> 'address' AS address,
-- now()
-- FROM
-- 	realtor_list_page_json

-- 找到有的propertyId
-- SELECT
-- 	rl."propertyId" AS "listPropertyId",
-- 	rl."lastUpdate" AS "listLastUpdate",
-- 	rl.address AS "listAddress",
-- 	rd."propertyId" AS "detailPropertyId",
-- 	rd."lastUpdate" AS "detailLastUpdate",
-- 	rd.address AS "detailAddress"
-- FROM
-- 	realtor_list_page_json_splite rl
-- 	INNER JOIN realtor_detail_page_json rd ON rl."propertyId" = rd."propertyId"


-- SELECT
-- 	rl."propertyId" AS "listPropertyId",
-- 	rl."lastUpdate" AS "listLastUpdate",
-- 	rl.address AS "listAddress",
-- 	rd."propertyId" AS "detailPropertyId",
-- 	rd."lastUpdate" AS "detailLastUpdate",
-- 	rd.address AS "detailAddress"
-- FROM
-- 	realtor_list_page_json_splite rl
-- 	INNER JOIN realtor_detail_page_json rd ON rl."propertyId" = rd."propertyId"
-- 	WHERE  rl."propertyId" is NOT null
-- 	 and rl."lastUpdate" is NOT NULL
-- 	 and rl.address is NOT NULL



-- UPDATE realtor_detail_page_json set "isDirty"='1'
-- WHERE "propertyId" ='6264702487'




-- 找到没有的propertyId

-- INSERT INTO realtor_detail_page_json( "propertyId", "lastUpdate", address,"isDirty")
-- (SELECT
-- 	rl."propertyId" AS "listPropertyId",
-- 	rl."lastUpdate" AS "listLastUpdate",
-- 	rl.address AS "listAddress",
-- 	0
-- -- -- -- -- 	rd."propertyId" AS "detailPropertyId",
-- -- -- -- -- 	rd."lastUpdate" AS "detailLastUpdate",
-- -- -- -- -- 	rd.address AS "detailAddress"
-- FROM
-- 	realtor_list_page_json_splite rl
-- 	left JOIN realtor_detail_page_json rd ON rl."propertyId" = rd."propertyId"
-- 	WHERE rd."propertyId" is NULL
-- 	and rl."propertyId" is NOT null
-- 	 and rl."lastUpdate" is NOT NULL
-- 	 and rl.address is NOT NULL
-- 	)
--




-- SELECT "propertyId" FROM realtor_detail_page_json
-- WHERE "detailJson" is NOT NULL
-- AND "isDirty" ='1'

-- UPDATE realtor_detail_page_json set "detailJson"='{"aa":"11"}' ,"isDirty"='0'
-- WHERE "propertyId" ='6264702487'
--
-- update realtor_detail_page_json set "optionDate"=now()




-- SELECT
-- 	"propertyId"
-- FROM
-- 	realtor_detail_page_json
-- WHERE
-- 	"isDirty" = '1'
-- 	OR "detailJson" IS NULL
--




--------------------------------------------------------:>>>>>>>>>>>>>>>>sql
-- INSERT INTO realtor_list_page_json_splite ( "propertyId", "lastUpdate", address ,"optionDate")
-- SELECT
-- json_array_elements ( "jsonData" -> 'listings' ) ->> 'property_id' AS "propertyId",
-- json_array_elements ( "jsonData" -> 'listings' ) ->> 'last_update' AS "lastUpdate",
-- json_array_elements ( "jsonData" -> 'listings' ) ->> 'address' AS address,
-- now()
-- FROM
-- 	realtor_list_page_json

-- 找到有的propertyId
-- SELECT
-- 	rl."propertyId" AS "listPropertyId",
-- 	rl."lastUpdate" AS "listLastUpdate",
-- 	rl.address AS "listAddress",
-- 	rd."propertyId" AS "detailPropertyId",
-- 	rd."lastUpdate" AS "detailLastUpdate",
-- 	rd.address AS "detailAddress"
-- FROM
-- 	realtor_list_page_json_splite rl
-- 	INNER JOIN realtor_detail_page_json rd ON rl."propertyId" = rd."propertyId" and rl."lastUpdate"=rd."lastUpdate"


-- SELECT
-- 	rl."propertyId" AS "listPropertyId",
-- 	rl."lastUpdate" AS "listLastUpdate",
-- 	rl.address AS "listAddress",
-- 	rd."propertyId" AS "detailPropertyId",
-- 	rd."lastUpdate" AS "detailLastUpdate",
-- 	rd.address AS "detailAddress"
-- FROM
-- 	realtor_list_page_json_splite rl
-- 	INNER JOIN realtor_detail_page_json rd ON rl."propertyId" = rd."propertyId"
-- 	WHERE  rl."propertyId" is NOT null
-- 	 and rl."lastUpdate" is NOT NULL
-- 	 and rl.address is NOT NULL



-- UPDATE realtor_detail_page_json set "isDirty"='1'
-- WHERE "propertyId" ='6264702487'




-- 找到没有的propertyId

-- INSERT INTO realtor_detail_page_json( "propertyId", "lastUpdate", address,"isDirty")
-- (SELECT
-- 	rl."propertyId" AS "listPropertyId",
-- 	rl."lastUpdate" AS "listLastUpdate",
-- 	rl.address AS "listAddress",
-- 	0
-- -- -- -- -- 	rd."propertyId" AS "detailPropertyId",
-- -- -- -- -- 	rd."lastUpdate" AS "detailLastUpdate",
-- -- -- -- -- 	rd.address AS "detailAddress"
-- FROM
-- 	realtor_list_page_json_splite rl
-- 	left JOIN realtor_detail_page_json rd ON rl."propertyId" = rd."propertyId"
-- 	WHERE rd."propertyId" is NULL
-- 	and rl."propertyId" is NOT null
-- 	 and rl."lastUpdate" is NOT NULL
-- 	 and rl.address is NOT NULL
-- 	)
--




-- SELECT "propertyId" FROM realtor_detail_page_json
-- WHERE "detailJson" is NOT NULL
-- AND "isDirty" ='1'

-- UPDATE realtor_detail_page_json set "detailJson"='{"aa":"11"}' ,"isDirty"='0'
-- WHERE "propertyId" ='6264702487'
--
-- update realtor_detail_page_json set "optionDate"=now()




-- SELECT
-- 	"propertyId"
-- FROM
-- 	realtor_detail_page_json
-- WHERE
-- 	"isDirty" = '1'
-- 	OR "detailJson" IS NULL
--
































































