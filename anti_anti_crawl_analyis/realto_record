    1：17.48:
    开始爬取，使用的是
    setting 中的配置情况：
        CONCURRENT_REQUESTS = 100
    # 降低log级别：降低到INFO级别就不能获取重定向的一些信息了
    # LOG_LEVEL = 'INFO'
    # 对于不需要登陆的网站禁用cookies
    COOKIES_ENABLED = False
    # 禁止重试:对于失败的http请求取消重试；但是这个还需要考虑
    RETRY_ENABLED = False

    # 如果您对一个非常慢的连接进行爬取(一般对通用爬虫来说并不重要)， 减小下载超时能让卡住的连接能被快速的放弃并解放处理其他站点的能力。
    DOWNLOAD_TIMEOUT = 15 # 其中15是设置的下载超时时间

    # 禁止重定向
    REDIRECT_ENABLED = False
    # 设置代理ip池，可以使用downloadermiddleware
    # 配置请求头


    # 自动限速设置：
    AUTOTHROTTLE_ENABLED = True
    DOWNLOAD_DELAY = 1
    2019-01-18 17:57:04 [scrapy.extensions.logstats] INFO: Crawled 75 pages (at 0 pages/min), scraped 36 items (at 0 items/min)

2：18.27
    100 并发去掉：（使用1）可能是fiddler的原因；
    2019-01-18 18:30:40 [scrapy.extensions.logstats] INFO: Crawled 67 pages (at 12 pages/min), scraped 32 items (at 5 items/min)
3：18.27：关掉fiddler代理


4:直接重构代码，详情页不用接口爬取；
    1:使用哈尔滨的网络，参数调到最低，没过多久就封ip了
    2：使用遨游vpn，使用美国的线路，相同参数效果还算稳定；
        能够跑完，但是数据量不够，不知道那里出问题了：
            找到原因了：xpath语法的原因；
        12：13开发抓的，使用最低配置：使用遨游，抓取时间：

总结：使用哈尔滨的ip不管是抓接口还是抓dom，美国都多就封了；（使用的还是最低的配置）抓了30多个item就封了

使用遨游vpn：
    各项参数调到最低也是直接封ip（抓了284）个
    2019-01-21 12:41:26 [scrapy.extensions.logstats] INFO: Crawled 316 pages (at 33 pages/min), scraped 284 items (at 12 items/min)
    反正抓取的时间不长；

    加上referer：一样的

    我怀疑是如果禁用了cookies之后对于一个ip会有用户个数限制，如果用户量过多，就会直接封ip

    # 加上一些别的浏览器标识，禁用cookie，自动限速看一下情况：
     Crawled 83 pages (at 39 pages/min), scraped 29 items (at 0 items/min)
     果然是针对用cookie级别的封锁；
     不同的ip有用户量的限制；


新的办法，
    1：使用无头浏览器，大量获取网站的cookies，然后封完一个cookies之后再使用另外一个cookies，这样把封锁限制到用户级别，
     <div class="alert alert-sm alert-warning alert-corner alert-dismissible no-show" role="alert" id="notification-cookies">
            <button type="button" class="close" data-dismiss="alert" aria-label="Close">
                <span aria-hidden="true">&times;</span>
            </button>
            <span>It looks like Cookies are disabled in your browser. For the best experience, please enable cookies when using our site.</span>
        </div>

看了一遍最新的文章发现scrapy有自动管理cookie的功能
但是使用cookie的时候也是同样发送几十个请求之后封了ip；（哈尔冰的网络）但是agent是动态的，
注释掉动态的agent；情况：
2019-01-22 16:04:07 [scrapy.extensions.logstats] INFO: Crawled 82 pages (at 37 pages/min), scraped 31 items (at 0 items/min)
情况一样；

目前能做的可能性就是用美国的网络：在启用cookie的情况下（在要被封之前就清除cookie，重新获取cookie）不然一被封就是ip被封，不是封一个cookie
这种情况下就要获取大量的cookie；（但是同一个ip下新的cookie过多也会被封，）
所以这个网站很恐怖，
参数调到最低的情况下依旧被封；
