trulia 网站是必须携带cookies和加载javascript的；
    不然会出现以下界面：
    Please verify you are a human
    Please click "I am not a robot" to continue

    Access to this page has been denied because we believe you are using automation tools to browse the website.

    This may happen as a result of the following:

    Javascript is disabled or blocked by an extension (ad blockers for example)
    Your browser does not support cookies
    Please make sure that Javascript and cookies are enabled on your browser and that you are not blocking them from loading.

    Reference ID: #a48cd830-1d4a-11e9-8917-87979c98f059

通过分析cookie发现：
    1：csrft是不变的参与到了cookie的合成中
    jA0ya2SbWv3cOtEpj66HYO+eAi5DCp0GGeJWxahT8pk=
    jA0ya2SbWv3cOtEpj66HYO%2BeAi5DCp0GGeJWxahT8pk%3D


    trulia 网站也和zillow网站有相似之处，数据显示不完全，也就是说还是要限制条件；
    https://www.trulia.com/NY/New_York/600_p/顶多显示600页，这还是数据多的情况，数据少的情况下不知道要多少也；
    所以需要设置限定条件：
    但是现在先不考虑这个情况，先测试有没有反爬；


trulia 这个网站很奇怪，有时候需要cookies，有时候不要cookies
trulia 网站cookie里面包含了搜索条件:

通过scrapy的debugcookies进行分析：

对于trulia网站，cookie是可带可不带得，（由于网站要做seo，不可能所有的都需要身份识别，但是
不带cookies时间长了会对这个：
    有点奇怪这里，不带cookie会被封，带cookie也会被封，还是没有找到正真原因；
    很奇怪，这个网站带着假的cookie也能通过；不知道是不是网站在这一方面有容错率，

    在注释掉cookies enable的时候scrapy每次携带过去的cookies都是一样的；

    这个网站可能存在着不够果断的情况：
    首先当我用scrapy去抓取数据的时候，如果抓挂了之后（是注释掉cookiesenable的情况）
        如果再经过浏览器访问该网站，那么他就会robot访问禁止的问题，如果再用新的cookies访问（假的）它依旧能访问到新的数据（使用的requests）库
        当时还再浏览器中点击了rotbot机器人的按钮，不知道是这种方式中的哪一种；

        可以做出如下假设：
            如果对于一个ip，如果检测出同一个ip下出现异常情况，他会短暂的封锁该ip，如果有人访问就跳出rotbot界面
            让用户做robot检测，如果检测之后，这个ip就有可以用scrapy继续抓取了，这是启动了cookie的情况下；
            那相比于realtor网站，这个网站没有对同一个ip下做流量异常检测；
            但是cookie异常检测应该是有的，再一次测试之中禁用cookie后，好像也很快就封了；


        目前有一种方式：就是更换cookies；因为它的屏蔽不是直接屏蔽的ip，而是屏蔽的一个cookie
        而且就算是屏蔽了一个cookie之后,过一会儿,哪个cookie也是可用,但是使用请求个数不长,250个请求就封闭了
        但是有时候就很稳定,说不准,
        还是要解决更换cookie的问题;

        还没有试过用假的cookie的效果,现在用的是浏览器中复制的cookies;还比较好用;在启用scrapy的自动限速的情况下还比较好用;
        能够持续抓取;


        昨天在直接抓房屋数据的时候,直接启用了cookie直接抓取,但是也没有做很大的限制;(比较有希望的应该就是这个网站了)

        但是要完美的搞定这个网站还需要:
            1:cookies的获取
            2:对于当遇到403的时候的处理方式
                1:将遇到的403的url重新yield到scrapy request中,但是这里的处理需要对scrapy比较熟
                2:使用信号量或者是使用middleware进行处理,或者是使用扩展;
                3:使用middleware话就用process_response,但是当遇到这个问题的时候怎样暂停爬虫,(爬虫是接受的ctrl+c 命令才能暂停爬虫,其他方式的停止不会保存状态;可以使用类似cmdline的方式)
                4:使用信号量,首先在官网没有看到相关信号有403信号的,其次还是关于爬虫暂停问题;

        trulia 网站一定程度上是针对cookie的封锁;亦或者是user-agent,

        但是有个很奇怪的现象就是用scrapy的自动限速功能,加上download_delay 会很容易出现403问题,(还没有抓到5000多条)
        但是不启用自动限速反而更好,我还以可能由于自动限速中的delay的原因,由于延迟固定,所以很有可能导致这样的问题,
        还是需要好好理解自动限速算法是怎样写的;

        再一次测试,使用默认,大约应该有1000多个请求被封了(不知道是时间上的限制还是,其他的,但是cookie是不可以在用了)
















